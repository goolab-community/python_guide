{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUY9csj1FRBQ"
   },
   "source": [
    "# პროგრამის ტესტირების მეთოდები \n",
    "\n",
    "ცხადია, იმის გასაგებად თუ როგორ მუშაობს პროგრამა და ასრულებს დასახულ ფუნქციებს, აუცილებელია მისი გამოცდა, რაც დამწყები პითონის პროგრამისტისთვის შემოიფარგლება პრინტის გამოყენებით. პრინტ ფუნქციით გატესტვა მისაღებია სანამ პროგრამა არ არის კომპლექსური და შედეგი ადვილად დასანახია, სხვა შემთხვევაში მიზანშეწონილია რაიმე ავტომატური მეთოდის მოფიქრება, რომლის საშუალებითაც ხელით არ მოგვიწევს ყოველი დეტალის და შემთხვევის გადამოწმება, პროგრამის უმნიშვნელო ცვლილების შემთხვევაშიც კი. ასეთი სტანდარტული ტესტები შეიძლება დაიწეროს, როგორც ცალკეული ფუნქციისთვის და კლასისთვის ისე მთელი მოდულისთვის. \n",
    "შესაბამისი ავტომატური ტესტების მეთოდები მრავალი წელია რაც დამკვიდრებულია და სტანდარტიზირებულია სხვადასხვა პროგრამირების ენაზე, მათ შორის პითონშიც გვაქვს ჩაშენებული unittest მოდული. გარდა unittest მოდულისა ასევე გავრცელებულია pytest. \n",
    "\n",
    "\n",
    "### unittest\n",
    "unittest მოდულის დახმარებით ტესტების დასაწერად აუცილებლია კლასის შექმნა რომლიც იქნება unittest მოდულის მემკვიდრე. მაგალითისთვის შევქმნათ ცალკე პითონის ტესტ ფაილი სადაც გაიწერება ტესტის ფუნქციები. აუცილებელია ვიცოდეთ, რომ სტანდარტულად ტესტების დაწერა რეკომენდირებულია ცალკე ფაილში და არა იმავე პროგრამის გვერდით ერთ ფაილში, ქვემოთ მოცემულ მაგალითში ტესტ კლასი გასატესტ კოდთან ერთადაა მოქცეული, რასაც ამ შემთხვევაში პრინციპული მნიშვნელობა არ აქვს და მხოლოდ მაგალითის გასამარტივებლადაა ნაჩვენები."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXVShI6LFRBT",
    "outputId": "cad29bed-2243-4906-a3b8-9fc9a0ede4d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_division (__main__.TestMath) ... ok\n",
      "test_square (__main__.TestMath) ... ok\n",
      "test_sum (__main__.TestMath) ... ok\n",
      "test_zero_division (__main__.TestMath) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.012s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# Start of the test section\n",
    "def sum(a, b):\n",
    "    return a + b\n",
    "\n",
    "def square(l):\n",
    "    return l**3\n",
    "\n",
    "def division(a, b):\n",
    "    if b == 0:\n",
    "        raise ZeroDivisionError(\"Can't divide by zero!\")\n",
    "    return a / b\n",
    "# End of the test section\n",
    "\n",
    "class TestMath(unittest.TestCase):\n",
    "    \n",
    "    def test_sum(self):\n",
    "        # Arrange\n",
    "        a = 0\n",
    "        b = 1\n",
    "        # Act\n",
    "        result = sum(a, b)\n",
    "        # Assert\n",
    "        self.assertEqual(result, a + b)\n",
    "        # ------\n",
    "        self.assertEqual(sum(1, 0.2), 1.2)\n",
    "        self.assertEqual(sum(4, 6), 10)\n",
    "    \n",
    "    def test_square(self):        \n",
    "        self.assertEqual(square(0), 0)\n",
    "        self.assertEqual(square(1), 1)\n",
    "        self.assertEqual(square(4), 64)\n",
    "        \n",
    "    def test_division(self):\n",
    "        self.assertEqual(division(1, 1), 1)\n",
    "        self.assertEqual(division(20, 5), 4)\n",
    "    \n",
    "    def test_zero_division(self):\n",
    "        #self.assertRaises(ZeroDivisionError, division, 4, 0)\n",
    "        # With context manager\n",
    "        with self.assertRaises(ZeroDivisionError):\n",
    "            division(4, 0)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=['/home/pyuser'], exit=False, verbosity=2)\n",
    "    #help(unittest.main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1BYityoFRBV"
   },
   "source": [
    "ადვილი მისახვედრია რომ ტესტის პროცესში მხოლოდ ის მეთოდები მიიღებენ მონაწილეობას რომელიც მიწყება test დასახელებით და გამოყოფილია ქვედა ხაზით, ხოლო ყველა დანარჩენი მეთოდი არ მიიღებს ავტომატური ტესტის პროცესში მონაწილეობას, გადა იმ შემთხვევისა როდესაც შეგნებულად გამოვიძახებთ. \n",
    "მაგალითში test_sum ფუნქცია თვალსაჩინოებისთვის დაყოფილია ტესტისთვი აუცილებელ სექციებად. პირველ სექციაში მოცემულია პარამეტრების მინიჭება, მეორეში ხდება გასატესტი ფუნქციის გაშვება, ხოლო ბოლო ეტაპზე assertEqual პარამეტრად გადაეცემა ფუნქციის მიერ დაბრუნებული შედეგი და სასურველი \"აუთფუთი\" (a + b). შემდგომ ფუნქციებში აღნიშნული მოქმედება გამარტივების მიზნით ხდება ერთ ხაზზში.\n",
    "assertEqual ფუნქცია ადარებს შედეგს, სასურველ მნიშვნელობასთან და იმ შემთხვაში თუ ერთმანეთისგან განსხვავდება, ტესტი ჩაიჭრება მოცემული ფუნციისთვის.  \n",
    "\n",
    "```python\n",
    "def test_sum(self):\n",
    "    # Arrange\n",
    "    a = 0\n",
    "    b = 1\n",
    "    # Act\n",
    "    result = sum(a, b)\n",
    "    # Assert\n",
    "    self.assertEqual(result, a + b)\n",
    "    # ------\n",
    "```\n",
    "\n",
    "თეორიულად ტესტის პროცესი დატყოფილია სამ ნაწილად:\n",
    "- Arrange - პარამეტრების მომზადება\n",
    "- Act     - სატესტო ფუნქციის გაშვება\n",
    "- Assert  - ტესტზე შემოწმება"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "გარდა ტესტ მეთოდებისა უნიტტესტ კლასში შეგვიძლია განვსაზღვროთ სხვადასხვა მეთოდები, რომლებიც მიბმულები იქნებიან პროგრამის შესრულების გარკვეულ პოზიციაზე (state). ქვემოთ მოცემულ მაგალითში თითოეულ მეთოდზე გაწერილია მისი აღწერა კომენტარის სახით."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sPt8UiuFRBW",
    "outputId": "468fe362-fb0e-436f-9723-273501e45abd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_division (__main__.TestMath) ... ok\n",
      "test_square (__main__.TestMath) ... ok\n",
      "test_sum (__main__.TestMath) ... ok\n",
      "test_zero_division (__main__.TestMath) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class set up\n",
      "Set up\n",
      "Tear down\n",
      "Set up\n",
      "Tear down\n",
      "Set up\n",
      "Test sum function\n",
      "Tear down\n",
      "Set up\n",
      "Tear down\n",
      "Class tear down\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.014s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# Start of the test section\n",
    "def sum(a, b):\n",
    "    return a + b\n",
    "\n",
    "def square(l):\n",
    "    return l**3\n",
    "\n",
    "def division(a, b):\n",
    "    if b == 0:\n",
    "        raise ZeroDivisionError(\"Can't divide by zero!\")\n",
    "    return a / b\n",
    "# End of the test section\n",
    "\n",
    "class TestMath(unittest.TestCase):\n",
    "    \n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # შესრულდება მხოლოდ ერთხელ, ობიექტის შექმნისას\n",
    "        print('Class set up')\n",
    "    \n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        # შესრულდება მხოლოდ ერთხელ, ობიექტის განადგურებისას\n",
    "        print('Class tear down')\n",
    "    \n",
    "    def setUp(self):\n",
    "        # შესრულდება ნებისმიერი ტესტ ფუინქციის გამოძახების წინ\n",
    "        print('Set up')\n",
    "        \n",
    "    def tearDown(self):\n",
    "        # შესრულდება ნებისმიერი ტესტ ფუინქციის შესრულების შემდეგ\n",
    "        print('Tear down')\n",
    "        \n",
    "    def test_sum(self):\n",
    "        print('Test sum function')\n",
    "        # Arrange\n",
    "        a = 0\n",
    "        b = 1\n",
    "        # Act\n",
    "        result = sum(a, b)\n",
    "        # Assert\n",
    "        self.assertEqual(result, a + b)\n",
    "        # ------\n",
    "        self.assertEqual(sum(1, 0.2), 1.2)\n",
    "        self.assertEqual(sum(4, 6), 10, 'უნდა იყოს 10')\n",
    "    \n",
    "    def test_square(self):        \n",
    "        self.assertEqual(square(0), 0)\n",
    "        self.assertEqual(square(1), 1)\n",
    "        self.assertEqual(square(4), 64)\n",
    "        \n",
    "    def test_division(self):\n",
    "        self.assertEqual(division(1, 1), 1)\n",
    "        self.assertEqual(division(20, 5), 4)\n",
    "    \n",
    "    def test_zero_division(self):\n",
    "        #self.assertRaises(ZeroDivisionError, division, 4, 0)\n",
    "        # With context manager\n",
    "        with self.assertRaises(ZeroDivisionError):\n",
    "            division(4, 0)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=['/home/pyuser'], exit=False, verbosity=2)\n",
    "    #help(unittest.main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XPEvktyFRBX"
   },
   "source": [
    "### pytest \n",
    "unittest-ისგან განსხვავებით pytest მოკლებულია ზედმეტი შაბლონური კოდის წერას და გაცილებით მოსახერხებელია, განსაკუთრებით იმ შემთხვევაში თუ ტესტის მოცულობა დიდია. შეიძლება ითქვას რომ pytest მოდული მეტად პითონურია ვიდრე unittest მოდული. ამასთან pytest მოდული გამოირჩევა შედარებით მეტი ავტომატური ტესტის შესაძლებლობით. მაგალითისთვის ზემოთ აღწერილი ტესტ მოდული შეგვიძლია გადავწეროთ pytest-ისთვის."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RvhhMVBvFRBY"
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "# Start of the test section\n",
    "def sum(a, b):\n",
    "    return a + b\n",
    "\n",
    "def square(l):\n",
    "    return l**3\n",
    "\n",
    "def division(a, b):\n",
    "    if b == 0:\n",
    "        raise ZeroDivisionError(\"Can't divide by zero!\")\n",
    "    return a / b\n",
    "# End of the test section\n",
    "\n",
    "def test_sum():\n",
    "    assert sum(0, 1) == (a + b)\n",
    "    assert sum(1, 0.2) == 1.2\n",
    "    assert sum(4, 6) == 10\n",
    "\n",
    "def test_square():        \n",
    "    assert square(0) == 0\n",
    "    assert square(1) == 1\n",
    "    assert square(4) == 64\n",
    "\n",
    "def test_division():\n",
    "    assert division(1, 1) == 1\n",
    "    assert division(20, 5) == 4\n",
    "\n",
    "def test_zero_division():\n",
    "    with pytest.raises(ZeroDivisionError):\n",
    "        division(4, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.install_and_configure.ipynb\r\n",
      "10.files_io_and_streams.ipynb\r\n",
      "11.type_hinting.ipynb\r\n",
      "12.decorators.ipynb\r\n",
      "13.docstring.ipynb\r\n",
      "14.unittest_and_pytest.ipynb\r\n",
      "15.async_await.ipynb\r\n",
      "16.generators_and_iterators.ipynb\r\n",
      "17.metaclases.ipynb\r\n",
      "18.using_getter_and_setter.ipynb\r\n",
      "19.publish_your_project_in_to_pypi.ipynb\r\n",
      "1.general_principles.ipynb\r\n",
      "2.math_and_logic.ipynb\r\n",
      "3.string_and_unicode.ipynb\r\n",
      "4.loops_and_conditional.ipynb\r\n",
      "5.data_structures.ipynb\r\n",
      "6.1.builtin_functions.ipynb\r\n",
      "6.functions.ipynb\r\n",
      "7.modules_and_packages.ipynb\r\n",
      "8.classes_and_inheritance.ipynb\r\n",
      "9.magic_methods.ipynb\r\n",
      "EventLoopPythonAsyncio.png\r\n",
      "ipycanvas.ipynb\r\n",
      "list_indexing.jpg\r\n",
      "pep8_in_short.ipynb\r\n",
      "Philosophy.ipynb\r\n",
      "python_in_one_page.ipynb\r\n",
      "Python_packaging.ipynb\r\n",
      "Python_patterns_and_anti_patterns.ipynb\r\n",
      "sqlachemy_101.pptx\r\n",
      "with_statement.ipynb\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7Vp6z4gFRBY"
   },
   "source": [
    "ვხედავთ რომ ზემოთ მოცემული ტესტის ჩანაწერი გაცილებით წაკითხვადია და მარტივია გასარჩევად. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAuQjp0IFRBY"
   },
   "source": [
    "ტესტის ფაილი შეგვძლია გავუშვათ შემდეგნაირად:\n",
    "\n",
    "``` bash\n",
    "pytest ./test_module.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-zAThGQFRBZ"
   },
   "source": [
    "* assert\n",
    "* pytest.raises\n",
    "* @pytest.fixture\n",
    "* Parametrized test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKETVv5NFRBZ"
   },
   "source": [
    "#### Option 1) Run tests by substring matching\n",
    "\n",
    "Here to run all the tests having method1 in its name we have to run\n",
    "\n",
    "```bash\n",
    "py.test -k method1 -v\n",
    "-k <expression> is used to represent the substring to match\n",
    "-v increases the verbosity\n",
    "So running py.test -k method1 -v will give you the following result\n",
    "\n",
    "test_sample2.py::test_file2_method1 FAILED\n",
    "test_sample1.py::test_file1_method1 FAILED\n",
    "\n",
    "============================== FAILURES ===================================\n",
    "_________________________ test_file2_method1 ______________________________\n",
    "    def test_file2_method1():\n",
    "    \tx=5\n",
    "    \ty=6\n",
    "       \tassert x+1 == y,\"test failed\"\n",
    ">      \tassert x == y,\"test failed because x=\" + str(x) + \" y=\" + str(y)\n",
    "E       AssertionError: test failed because x=5 y=6\n",
    "E       assert 5 == 6\n",
    "test_sample2.py:5: AssertionError\n",
    "\n",
    "__________________________ test_file1_method1 ____________________________\n",
    "    @pytest.mark.only\n",
    "    def test_file1_method1():\n",
    "    \tx=5\n",
    "    \ty=6\n",
    "       \tassert x+1 == y,\"test failed\"\n",
    ">      \tassert x == y,\"test failed because x=\" + str(x) + \" y=\" + str(y)\n",
    "E       AssertionError: test failed because x=5 y=6\n",
    "E       assert 5 == 6\n",
    "test_sample1.py:8: AssertionError\n",
    "\n",
    "================== 2 tests deselected by '-kmethod1' ==================\n",
    "================== 2 failed, 2 deselected in 0.02 seconds =============\n",
    "Here you can see towards the end 2 tests deselected by '-kmethod1' which are test_file1_method2 and test_file2_method2\n",
    "```\n",
    "Try running with various combinations like:-\n",
    "``` bash\n",
    "py.test -k method -v - will run all the four methods\n",
    "py.test -k methods -v – will not run any test as there is no test name matches the substring 'methods'\n",
    "```\n",
    "#### Option 2) Run tests by markers\n",
    "    \n",
    "Pytest allows us to set various attributes for the test methods using pytest markers, @pytest.mark . To use markers in the test file, we need to import pytest on the test files.\n",
    "\n",
    "Here we will apply different marker names to test methods and run specific tests based on marker names. We can define the markers on each test names by using\n",
    "\n",
    "`@pytest.mark.<name>.`\t\t\t\n",
    "We are defining markers set1 and set2 on the test methods, and we will run the test using the marker names. Update the test files with the following code\n",
    "\n",
    "``` python\n",
    "#test_sample1.py\n",
    "\n",
    "import pytest\n",
    "@pytest.mark.set1\n",
    "def test_file1_method1():\n",
    "\tx=5\n",
    "\ty=6\n",
    "\tassert x+1 == y,\"test failed\"\n",
    "\tassert x == y,\"test failed because x=\" + str(x) + \" y=\" + str(y)\n",
    "\n",
    "@pytest.mark.set2\n",
    "def test_file1_method2():\n",
    "\tx=5\n",
    "\ty=6\n",
    "\tassert x+1 == y,\"test failed\"\n",
    "test_sample2.py\n",
    "\n",
    "import pytest\n",
    "@pytest.mark.set1\n",
    "def test_file2_method1():\n",
    "\tx=5\n",
    "\ty=6\n",
    "\tassert x+1 == y,\"test failed\"\n",
    "\tassert x == y,\"test failed because x=\" + str(x) + \" y=\" + str(y)\n",
    "\n",
    "@pytest.mark.set1\n",
    "def test_file2_method2():\n",
    "\tx=5\n",
    "\ty=6\n",
    "\tassert x+1 == y,\"test failed\"\n",
    "```\n",
    "We can run the marked test by\n",
    "\n",
    "`py.test -m <name>`\n",
    "`-m <name>` mentions the marker name\n",
    "Run py.test -m set1.This will run the methods test_file1_method1, test_file2_method1, test_file2_method2.\n",
    "\n",
    "Running py.test -m set2 will run test_file1_method2.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7QZG3oiFRBa"
   },
   "source": [
    "### Substring Matching of Test Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRZfnHc3FRBa"
   },
   "source": [
    "To execute the tests containing a string in its name we can use the following syntax −\n",
    "\n",
    "`pytest -k <substring> -v`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhE_xQ1WFRBb"
   },
   "source": [
    "### Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lAl-7QIFRBb"
   },
   "source": [
    "Fixtures are functions, which will run before each test function to which it is applied. Fixtures are used to feed some data to the tests such as database connections, URLs to test and some sort of input data. Therefore, instead of running the same code for every test, we can attach fixture function to the tests and it will run and return the data to the test before executing each test.\n",
    "\n",
    "A function is marked as a fixture by −\n",
    "\n",
    "@pytest.fixture\n",
    "A test function can use a fixture by mentioning the fixture name as an input parameter.\n",
    "\n",
    "Create a file test_div_by_3_6.py and add the below code to it\n",
    "``` python\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def input_value():\n",
    "   input = 39\n",
    "   return input\n",
    "\n",
    "def test_divisible_by_3(input_value):\n",
    "   assert input_value % 3 == 0\n",
    "\n",
    "def test_divisible_by_6(input_value):\n",
    "   assert input_value % 6 == 0\n",
    "```\n",
    "\n",
    "Here, we have a fixture function named input_value, which supplies the input to the tests. To access the fixture function, the tests have to mention the fixture name as input parameter.\n",
    "\n",
    "Pytest while the test is getting executed, will see the fixture name as input parameter. It then executes the fixture function and the returned value is stored to the input parameter, which can be used by the test.\n",
    "\n",
    "Execute the test using the following command −\n",
    "\n",
    "`pytest -k divisible -v`\n",
    "The above command will generate the following result −\n",
    "``` bash\n",
    "test_div_by_3_6.py::test_divisible_by_3 PASSED\n",
    "test_div_by_3_6.py::test_divisible_by_6 FAILED\n",
    "============================ FAILURES ============================\n",
    "________________________________________ test_divisible_by_6\n",
    "_________________________________________\n",
    "input_value = 39\n",
    "   def test_divisible_by_6(input_value):\n",
    ">  assert input_value % 6 == 0\n",
    "E  assert (39 % 6) == 0\n",
    "test_div_by_3_6.py:12: AssertionError\n",
    "========================== 1 failed, 1 passed, 6 deselected in 0.07 seconds ==========================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVq8dlz_FRBb"
   },
   "source": [
    "### When to Avoid Fixtures\n",
    "Fixtures are great for extracting data or objects that you use across multiple tests. They aren’t always as good for tests that require slight variations in the data. Littering your test suite with fixtures is no better than littering it with plain data or objects. It might even be worse because of the added layer of indirection.\n",
    "\n",
    "As with most abstractions, it takes some practice and thought to find the right level of fixture use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-H99CaqFRBc"
   },
   "source": [
    "### Parametrization: Combining Tests\n",
    "\n",
    "You saw earlier in this tutorial how pytest fixtures can be used to reduce code duplication by extracting common dependencies. Fixtures aren’t quite as useful when you have several tests with slightly different inputs and expected outputs. In these cases, you can parametrize a single test definition, and pytest will create variants of the test for you with the parameters you specify.\n",
    "\n",
    "Imagine you’ve written a function to tell if a string is a palindrome. An initial set of tests could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1638422822060,
     "user": {
      "displayName": "Goolab Georgia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjLwARfioD_kpl_CRagGioVJiKKFYK7h_9spFmD=s64",
      "userId": "15658151831524475469"
     },
     "user_tz": -240
    },
    "id": "aT6jsEcwFRBc"
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "def is_palindrome(st):\n",
    "    def strp(s, args):\n",
    "        for n in args:\n",
    "            s = s.strip(n)\n",
    "        return s\n",
    "    s = st.lower().replace(' ', '')\n",
    "    s = strp(s, ['?', '.', ',', ''])\n",
    "    l = len(s)\n",
    "    if l < 2:\n",
    "        return True\n",
    "    od = 0\n",
    "    if l % 2 == 1:\n",
    "        od = 1\n",
    "    if s[:l//2] == s[l//2+od:][::-1]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"maybe_palindrome, expected_result\", [\n",
    "    (\"\", True),\n",
    "    (\"a\", True),\n",
    "    (\"Bob\", True),\n",
    "    (\"Never odd or even\", True),\n",
    "    (\"Do geese see God?\", True),\n",
    "    (\"abc\", False),\n",
    "    (\"abab\", False),\n",
    "])\n",
    "def test_is_palindrome(maybe_palindrome, expected_result):\n",
    "    assert is_palindrome(maybe_palindrome) == expected_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovon-NpjFRBc"
   },
   "source": [
    "### Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhAWhH3nFRBc"
   },
   "source": [
    "```bash\n",
    "pytest --durations=3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# რესურსები\n",
    "\n",
    "- [Python Testing 101 with pytest](https://www.youtube.com/watch?v=etosV2IWBF0)\n",
    "- [ Python Testing 201 with pytest ](https://www.youtube.com/watch?v=fv259R38gqc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s9JJt4CnGtH"
   },
   "source": [
    "# PyTest ---------- new tutorial ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytest.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing exeptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(ZeroDivisionError):\n",
    "    assert 2 == 3/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indirect fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize('a, expected', [(0, 0), (1, 1), (4, 64), (5, 125)])\n",
    "def test_square(a, expected):\n",
    "    assert square(a) == expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content of test_class.py\n",
    "class TestClass:\n",
    "    def test_one(self):\n",
    "        x = \"this\"\n",
    "        assert \"h\" in x\n",
    "\n",
    "    def test_two(self):\n",
    "        x = \"hello\"\n",
    "        assert hasattr(x, \"check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixtures in configuration file\n",
    "Create file conftest.py for pytest configuration and defineing fixtures, fixture example will be like:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def my_fixture():\n",
    "    return 'hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call fixture with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fixrture1(my_fixture):\n",
    "    assert 'hello' == my_fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_capsys(capsys):\n",
    "    print(\"something\")\n",
    "    out, err = capsys.readouterr()\n",
    "    assert 'something\\n' == out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alter behaviour on run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_monkeypatch(monkeypatch):\n",
    "    def fake_sum(a, b):\n",
    "        return 123\n",
    "    monkeypatch.setattr(object_name, \"obj_method\", fake_sum)\n",
    "    assert object_name.obj_method(4, 1) == 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytest tmpdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tmpdir(tmpdir):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "14. Unittest and pytest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
